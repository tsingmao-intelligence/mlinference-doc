# 前端服务API交互接口标准

前端服务API的设计理念是兼容[openai](https://platform.openai.com/docs/api-reference/chat/create)的接口协议。以下是支持的特性和接口定义，部分描述摘抄自openai官方文档：[https://platform.openai.com/docs/guides/text-generation/chat-completions-api](https://platform.openai.com/docs/guides/text-generation/chat-completions-api)

## Completion

该接口用于模型内容补全。

### Endpoint

`POST http://127.0.0.1:port/completion`

### Request Body

#### ~~model \[won't support] (string) \[no use currently]~~

~~ID of the model to use.~~

#### prompt \[v0.1] (string or array, Required)

The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays. Note that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.

#### best\_of \[v0.3] (integer or null, Optional)

Defaults to 1. Generates `best_of` completions server-side and returns the "best" (the one with the highest log probability per token). Results cannot be streamed.

When used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return – `best_of` must be greater than `n`.

**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.

#### echo \[v0.1] (boolean or null, Optional)

Defaults to false. Echo back the prompt in addition to the completion

#### frequency\_penalty \[v0.2] (number or null, Optional)

Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.

#### logit\_bias \[v0.4] (map, Optional)

Defaults to null. Modify the likelihood of specified tokens appearing in the completion.

Accepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use our tokenization tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.

As an example, you can pass `{"50256": -100}` to prevent the <|endoftext|> token from being generated.

#### logprobs \[v0.4] (integer or null, Optional)

Defaults to null. Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.

The maximum value for `logprobs` is 5.

#### max\_tokens \[v0.1] (integer or null, Optional)

Defaults to 16. The maximum number of tokens that can be generated in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length.

#### n \[v0.3] (integer or null, Optional)

Defaults to 1. How many completions to generate for each prompt.

**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.

#### presence\_penalty \[v0.2] (number or null, Optional)

Defaults to 0. Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.

#### seed \[TBD] (integer or null, Optional)

If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.

Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.

#### stop \[v0.1] (string / array / null, Optional)

Defaults to null. Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.

#### stream \[v0.1] (boolean or null, Optional)

Defaults to false. Whether to stream back partial progress. If set, tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent\_events/Using\_server-sent\_events#Event\_stream\_format) as they become available, with the stream terminated by a `data: [DONE]` message.

#### stream\_options \[v0.1] (object or null, Optional)

Defaults to null. Options for streaming response. Only set this when you set `stream: true`.

* include\_usage (boolean, Optional)\
  If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value.

#### ww \[v0.1] (number or null, Optional)

Defaults to 1. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.

#### top\_p \[v0.1] (number or null, Optional)

Defaults to 1. An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both.

#### do\_sample \[v0.1] (bool, Optional, default false)

控制是否进行sample

#### ~~user \[won't support] (string, Optional)~~

~~A unique identifier representing your end-user, which can help monitor and detect abuse.~~

### Returns

Returns a [#completion-object](qian-duan-fu-wu-api-jiao-hu-jie-kou-biao-zhun.md#completion-object "mention"), or a sequence of [#completion-object](qian-duan-fu-wu-api-jiao-hu-jie-kou-biao-zhun.md#completion-object "mention")s if the request is streamed.

### Completion Object

Represents a completion response from the API. Note: both the streamed and non-streamed response objects share the same shape (unlike the chat endpoint).

#### id \[v0.1] (string)

A unique identifier for the completion.

#### choices \[v0.1] (array)

The list of completion choices the model generated for the input prompt.

* finish\_reason \[v0.1] (string)\
  The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, or `content_filter(TBD?)` if content was omitted due to a flag from our content filters.
* index \[v0.1] (integer)
* logprobs \[v0.4] (object or null)
  * text\_offset (array)
  * token\_logprobs (array)
  * tokens (array)
  * top\_logprobs (array)
* text \[v0.1] (string)

#### created \[v0.1] (integer)

The Unix timestamp (in seconds) of when the completion was created.

#### model \[v0.1] (string)

The model used for completion.

#### system\_fingerprint \[TBD] (string)

This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.

#### object \[v0.1] (string)

The object type, which is always "text\_completion"

#### usage \[v0.1] (object)

Usage statistics for the completion request.

* completion\_tokens (integer)\
  Number of tokens in the generated completion.
* prompt\_tokens (integer)\
  Number of tokens in the prompt.
* total\_tokens (integer)\
  Total number of tokens used in the request (prompt + completion).

## Chat

该接口用于对话。

### <mark style="color:red;">Endpoint</mark>

`POST http://127.0.0.1:port/chat`

### <mark style="color:red;">Request body</mark>

* **messages** (array): 到目前为止对话中的消息列表。如下为可支持的四类消息：

**System message \[v0.1]**

* **content** (string): 系统消息的内容。
* **role** (string): 角色类型，固定为 `system`。
* **name** (string, Optional): 参与者的可选名称。\[won't support]

**User message \[v0.1]**

* **content** (string 或 array): 用户消息的内容。
* **role** (string): 角色类型，固定为 `user`。
* **name** (string, Optional): 参与者的可选名称。\[won't support]

**Assistant message \[v0.1]**

* **content** (string 或 null): 助手消息的内容。
* **role** (string): 角色类型，固定为 `assistant`。
* **name** (string, Optional): 参与者的可选名称。\[won't support]
* **tool\_calls \[v0.2]** (array, Optional): 模型生成的工具调用，例如函数调用。

**Tool message \[v0.2]**

* **role** (string): 角色类型，固定为 `tool`。
* **content** (string): 工具消息的内容。
* **tool\_call\_id** (string): 此消息响应的工具调用。

### <mark style="color:red;">Returns</mark>

返回一个[Chat Completion](qian-duan-fu-wu-api-jiao-hu-jie-kou-biao-zhun.md#chat-completion-object)对象，如果请求是流式的，则返回流式的[chat stream completion](qian-duan-fu-wu-api-jiao-hu-jie-kou-biao-zhun.md#chat-completion-stream-object)对象。

### <mark style="color:red;">Other parameters</mark>

* ~~**model \[won't support]** (string): 要使用的模型 ID。~~
* **frequency\_penalty \[v0.2]** (number 或 null, Optional): 介于 -2.0 到 2.0 之间的数字。正值将基于现有文本中的频率对新标记进行惩罚，降低模型重复相同行的可能性。
* **logit\_bias \[v0.4]** (map, Optional): 修改特定标记出现的可能。接受一个 JSON 对象，将标记（由分词器中的标记 ID 指定）映射到 -100 到 100 的相关偏见值。
* **logprobs \[v0.4]** (boolean 或 null, Optional): 是否返回输出标记的对数概率。
* **top\_logprobs \[v0.4]** (integer 或 null, Optional): 一个介于 0 和 20 之间的整数，指定在每个标记位置返回最有可能的标记数量，每个标记都有相关的对数概率。
* **max\_tokens \[v0.1]** (integer 或 null, Optional): 聊天完成可以生成的最大数量。
* **n \[v0.3]** (integer 或 null, Optional): 为每个输入消息生成多少个聊天完成选项。注意，您将根据所有选项生成的标记数量计费。保持 n 为 1 以最小化成本。
* **presence\_penalty \[v0.2]** (number 或 null, Optional): 介于 -2.0 和 2.0 之间的数字。正值将基于它们在文本中的出现情况对新标记进行惩罚，增加模型讨论新话题的可能性。
* ~~**response\_format \[won't support]** (object, Optional): 指定模型必须输出的格式。与 GPT-4 Turbo 和所有 GPT-3.5 Turbo 模型兼容。~~
* **seed \[TBD]** (integer 或 null, Optional): 如果指定，我们的系统将尽力进行确定性采样，这样使用相同的 seed 和参数的重复请求将返回相同的结果。\[暂未支持，无实际作用]
* **stop \[v0.1]** (string / array / null, Optional): 至多 4 个序列，API 将在这些序列处停止生成更多标记。
* **stream \[v0.1]** (boolean 或 null, Optional): 如果设置，将发送部分消息增量，像 ChatGPT 一样。标记将作为数据唯一的 [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent\_events/Using\_server-sent\_events#Event\_stream\_format) 实时发送。
* **stream\_options \[v0.1]** (object 或 null, Optional): 流式响应的选项。仅在设置 `stream: true` 时设置。
  * **include\_usage** (boolean, Optional): If set, an additional chunk will be streamed before the `data: [DONE]` message. The `usage` field on this chunk shows the token usage statistics for the entire request, and the `choices` field will always be an empty array. All other chunks will also include a `usage` field, but with a null value.
* **temperature \[v0.1]** (number 或 null, Optional): 使用的采样温度，介于 0 和 2 之间。较高的值如 0.8 将使输出更加随机，而较低的值如 0.2 将使其更加专注和确定性。不建议和top\_p一起使用。
* **top\_p \[v0.1]** (number 或 null, Optional): 温度采样的替代方案，称为核心采样，模型考虑 top\_p 概率质量的标记结果。不建议和top\_p一起使用。
* **tools \[v0.2]** (array, Optional): 模型可能调用的工具列表。目前，只支持将函数作为工具。使用此功能提供模型可能生成 JSON 输入的函数列表。最多支持128个tools。
  * type (string, Required): The type of the tool. Currently, only `function` is supported.
  * function (object, Required)
    * string (Optional): A description of what the function does, used by the model to choose when and how to call the function.
    * name (string, Required): The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
    * parameters (object, Optional): The parameters the functions accepts, described as a JSON Schema object. See the [guide](https://platform.openai.com/docs/guides/function-calling) for examples, and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format. \
      Omitting `parameters` defines a function with an empty parameter list.
*   **tool\_choice \[v0.2]** (string 或 object, Optional): 控制模型调用哪个（如果有的话）工具。`none` means the model will not call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools. Specifying a particular tool via `{"type": "function", "function": {"name": "my_function"}}` forces the model to call that tool.

    `none` is the default when no tools are present. `auto` is the default if tools are present.

    * string: `none` means the model will not call any tool and instead generates a message. `auto` means the model can pick between generating a message or calling one or more tools. `required` means the model must call one or more tools.
    * object: Specifies a tool the model should use. Use to force the model to call a specific function.
      * type (string, Required): The type of the tool. Currently, only `function` is supported.
      * function (object, Required)
        * name (string, Required): The name of the function to call.
* **parallel\_tool\_calls \[v0.2]** (boolean, Optional): 是否在工具使用期间启用并行函数调用。
* ~~**user \[won't support]** (string, Optional): 表示您的最终用户的唯一标识符，这可以帮助 OpenAI 监控和检测滥用行为。~~

### Chat Completion Object

**id \[v0.1]** (string)\
A unique identifier for the chat completion.

**choices \[v0.1]** (array)\
A list of chat completion choices. Can be more than one if `n` is greater than 1.

* **finish\_reason \[v0.1]** (string)\
  The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
* **index \[v0.1]** (integer)\
  The index of the choice in the list of choices.
* **message \[v0.1]** (object)\
  A chat completion message generated by the model.
  * content (string or null)\
    The contents of the message.
  * tool\_calls \[v0.2] (array)\
    The tool calls generated by the model, such as function calls.
    * id (string)\
      The ID of the tool call.
    * type (string)\
      The type of the tool. Currently, only `function` is supported.
    * function (object)\
      The function that the model called.
      * name (string)\
        The name of the function to call.
      * arguments (string)\
        The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.

**role \[v0.1] (string)**\
The role of the author of this message.

**logprobs \[v0.4] (object or null)**\
Log probability information for the choice.

* content (array or null)\
  A list of message content tokens with log probability information.
  * token (string)\
    The token.
  * logprob (number)\
    The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
  * bytes (array or null)\
    A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
  * top\_logprobs (array)\
    List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
    * token (string)\
      The token.
    * logprob (number)\
      The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    * bytes (array or null)\
      A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.

**created \[v0.1] (integer)**\
The Unix timestamp (in seconds) of when the chat completion was created.

**model \[v0.1] (string)**\
The model used for the chat completion.

**system\_fingerprint \[TBD] (string)**\
This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.

**object \[v0.1] (string)**\
The object type, which is always `chat.completion`.

**usage \[v0.1] (object)**\
Usage statistics for the completion request.

* **completion\_tokens (integer)**\
  Number of tokens in the generated completion.
* **prompt\_tokens (integer)**\
  Number of tokens in the prompt.
* **total\_tokens (integer)**\
  Total number of tokens used in the request (prompt + completion).

### Chat Completion Stream Object

Represents a streamed chunk of a chat completion response returned by model, based on the provided input.

#### id \[v0.1] (string)

A unique identifier for the chat completion. Each chunk has the same ID.

#### choices \[v0.1] (array)

A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the last chunk if you set `stream_options: {"include_usage": true}`.

* delta \[v0.1] (object)\
  A chat completion delta generated by streamed model responses.
  * content \[v0.1] (string or null)\
    The contents of the chunk message.
  * tool\_calls \[v0.2] (array)
    * index (integer)
    * id (string)\
      The ID of the tool call.
    * type (string)\
      The type of the tool. Currently, only `function` is supported.
    * function (object)
      * name (string)\
        The name of the function to call.
      * arguments (string)\
        The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
  * role \[v0.1] (string)\
    The role of the author of this message.
* logprobs \[v0.4] (object or null)\
  Log probability information for the choice.
  * content (array or null)\
    A list of message content tokens with log probability information.
    * token (string)\
      The token.
    * logprob (number)\
      The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
    * bytes (array or null)\
      A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
    * top\_logprobs (array)\
      List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned.
      * token (string)\
        The token.
      * logprob (number)\
        The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely.
      * bytes (array or null)\
        A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.
* finish\_reason \[v0.1] (string or null)\
  The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence, `length` if the maximum number of tokens specified in the request was reached, `content_filter` if content was omitted due to a flag from our content filters, `tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.
* index \[v0.1] (integer)\
  The index of the choice in the list of choices.

#### created \[v0.1] (integer)

The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.

#### model \[v0.1] (string)

The model to generate the completion.

#### system\_fingerprint \[TBD] (string)

This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.

#### object \[v0.1] (string)

The object type, which is always `chat.completion.chunk`.

#### usage \[v0.1] (object)

An optional field that will only be present when you set `stream_options: {"include_usage": true}` in your request. When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request.

* completion\_tokens (integer)\
  Number of tokens in the generated completion.
* prompt\_tokens (integer)\
  Number of tokens in the prompt.
* total\_tokens (integer)\
  Total number of tokens used in the request (prompt + completion).
